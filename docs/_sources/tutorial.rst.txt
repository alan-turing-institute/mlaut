Tutorial
========

Basic Usage
-----------

The code below will help you get started with mlaut.

The following Jupyter Notebook contains the code used below.

:download:`mlaut_Basic_Usage.ipynb <../examples/mlaut_Basic_Usage.ipynb>`.


**1. Get the data and organise it.**

The enclosed code can be used as follows:  

.. literalinclude:: ../examples/basic.py
    :language: python
    :lines: 23-36

**2. Create the HDF5 database and establish hooks to the datasets.**

.. literalinclude:: ../examples/basic.py
    :language: python
    :lines: 38-44

:`aids_data`: hook to the aids dataset.
:`uis_data`:   hook to the uis dataset.


In this example we establish individual hooks to each of the datasets. However, MLaut has functionality to automate this process.
**3. Orchestrate the experiments.**

.. literalinclude:: ../examples/basic.py
    :language: python
    :lines: 47-57

:`cv`: cross validation object.
:`datasets`: list with the datasets that we will run the experiments on.
:`strategies`: list of machine learning strategies that we will on the datasets.


**4. Analyse the results of the experiments.**

The last step in the pipeline is to analyze the results of the experiments.

.. literalinclude:: ../examples/basic.py
    :language: python
    :lines: 60-64

The `prediction_errors()` method retuns two sets of results: `errors_per_estimator` dictionary which is used subsequently in further statistical tests and `errors_per_dataset_per_estimator_df` which is a dataframe with the loss of each estimator on each dataset which can be examined directly by the user. 

In this example we used accuracy as a loss function for calculating the prediction errors. However, MLaut supports a large number of loss functions, including user defined ones.


The `errors_per_estimator` result can be used as input to the following functions to perform the different statistical tests supported by ``mlaut``.

* :meth:`mlaut.experiments.analysis.ranks`
* :meth:`mlaut.experiments.analysis.t_test`
* :meth:`mlaut.experiments.analysis.sign_test`
* :meth:`mlaut.experiments.analysis.ranksum_test`
* :meth:`mlaut.experiments.analysis.t_test_with_bonferroni_correction`
* :meth:`mlaut.experiments.analysis.wilcoxon_test`
* :meth:`mlaut.experiments.analysis.friedman_test`
* :meth:`mlaut.experiments.analysis.nemenyi`

Please refer to :ref:`experiments` for additional information about their use.

MLAUT Large Scale ML Benchmarking Study
---------------------------------------

We used ``mlaut`` to perform a large scale benchmarking study for a number of machine learning models. In doing so we recreated and expanded a previous paper (:cite:`fernandez-delgado_we_2014`).

The code for our study can be downloaded from the following link :download:`mlaut_study <../examples/mlaut_study.zip>` and can also be used as an advanced tutorial and reference for using ``mlaut``.


.. bibliography:: references.bib


Docker Implementation
---------------------

Because the experiments can take quite a long time to complete we recommend running them inside a Docker container. For this purpose we have provided a Dockerfile that you can find here: :download:`Dockerfile <../Docker/Dockerfile>`.


First you should build the image:

.. code-block:: bash

    $ sudo docker build -t mlaut-train .

In order to run the experiments the user should mount a local directory to the `/mlaut` directory inside the container and execute a pre-defined script that sets up the experiments. Using the previous example from the benchmarking study:

.. code-block:: bash

    $ sudo docker run -ti -d --name mlaut-train -v "$(pwd):/mlaut" mlaut:latest /bin/sh -c 'python MLAUT_study.py'

For further information about using Docker please refer to its `documentation <https://www.docker.com/>`_.
